{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc51b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c1853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "from pathy import Pathy\n",
    "\n",
    "parser = argparse.ArgumentParser(description='AG training arguments')\n",
    "parser.add_argument('run_name')\n",
    "parser.add_argument('--train_datasets', nargs='+', help='Training dataset names')\n",
    "parser.add_argument('--test_datasets', nargs='+', help='Testing dataset names')\n",
    "parser.add_argument('-c', '--colab', default=False, action='store_true', help='Enable if using colab environment')\n",
    "parser.add_argument('-s', '--data_source', default='DRIVE', help='Source of training data')\n",
    "parser.add_argument('-d', '--device', default='TPU', help='Hardware device to train on')\n",
    "parser.add_argument('-b', '--batch_size', default=16, type=int)\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=2e-4)\n",
    "parser.add_argument('-t', '--train_steps', type=int, default=1000)\n",
    "parser.add_argument('--bucket_name', help='GCS bucket name to stream data from')\n",
    "parser.add_argument('--tpu_name', help='GCP TPU name') # Only used in the script on GCP\n",
    "\n",
    "\n",
    "\n",
    "### Sample local config\n",
    "args = parser.parse_args('''\n",
    "anglesrgb\n",
    "--train_dataset rgb_ppt/train\n",
    "--test_dataset rgb_ppt/val\n",
    "-c\n",
    "-s GCS\n",
    "--bucket_name lfp_europe_west4_a\n",
    "'''.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74754a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE = 'data/custom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2e7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"custom_train.json\" if train else \"custom_val.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"target\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7b47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Based on the class defined above, we create training and validation datasets.\n",
    "from transformers import DetrFeatureExtractor\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=f'{DATA_BASE}/train', feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=f'{DATA_BASE}/val', feature_extractor=feature_extractor, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5efea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 90\n",
      "Number of validation examples: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2c5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try this:   gsutil cp gs://lfp_europe_west4_a/saved_models/anglesrgb saved_models/\n"
     ]
    }
   ],
   "source": [
    "import subprocess \n",
    "\n",
    "def dld_model():\n",
    "    STORAGE_PATH = Pathy(f'gs://{args.bucket_name}')\n",
    "    command = [\"gsutil\", \"cp\", str(STORAGE_PATH/f'saved_models/{args.run_name}'), \"saved_models/\"]\n",
    "    \n",
    "    try:\n",
    "        subprocess.call(command)\n",
    "    except:\n",
    "        print(\"try this:  \", \" \".join(command))\n",
    "dld_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ac12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig\n",
    "from lib.DETR import DetrForObjectDetection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b96019",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "config = DetrConfig.from_pretrained(\"facebook/detr-resnet-50\", num_labels=len(id2label))\n",
    "model = DetrForObjectDetection(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cdf6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e0d1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DetrForObjectDetection:\n\tsize mismatch for class_labels_classifier.weight: copying a param with shape torch.Size([14, 256]) from checkpoint, the shape in current model is torch.Size([16, 256]).\n\tsize mismatch for class_labels_classifier.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aad3f5272199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-aad3f5272199>\u001b[0m in \u001b[0;36mload\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'saved_models/{args.run_name}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DetrForObjectDetection:\n\tsize mismatch for class_labels_classifier.weight: copying a param with shape torch.Size([14, 256]) from checkpoint, the shape in current model is torch.Size([16, 256]).\n\tsize mismatch for class_labels_classifier.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([16])."
     ]
    }
   ],
   "source": [
    "def load(model):\n",
    "    state_dict = torch.load(f'saved_models/{args.run_name}', map_location=torch.device(device))\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "load(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import math\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0,0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l/2.0, w/2.0), (l/2.0, -w/2.0), (-l/2.0, -w/2.0), (-l/2.0, w/2.0)]\n",
    "    return [(c*x-s*y+offset[0], s*x+c*y+offset[1]) for (x,y) in rectCoords]\n",
    "\n",
    "def plot_results(pil_img, prob, boxes, fills, rotations):\n",
    "    #plt.figure(figsize=(16,10))\n",
    "    #plt.imshow(pil_img)\n",
    "    #ax = plt.gca()\n",
    "    #colors = COLORS * 100\n",
    "    draw = ImageDraw.Draw(pil_img, \"RGBA\")\n",
    "    for p, (xmin, ymin, xmax, ymax), (r,g,b), (v1,v2) in zip(prob, boxes.tolist(), fills.tolist(), rotations.tolist()):\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "#         ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "#                                    fill=False, color=np.array([r,g,b]), linewidth=3))\n",
    "        A  = np.arctan2(v2,v1)\n",
    "        vertices = makeRectangle(w, h, A, offset=(xmin+w/2, ymin+h/2))  \n",
    "        draw.polygon(vertices, outline='red')\n",
    "        cl = p.argmax()\n",
    "        text = f'{id2label[cl.item()]}: {p[cl]:0.2f} - {np.array([r,g,b])}'\n",
    "        draw.text((vertices[2][0], vertices[2][1]), text, fill='black')\n",
    "#         ax.text(xmin, ymin, text, fontsize=15,\n",
    "#                 bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    return pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(image, outputs, threshold):\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "\n",
    "    # convert predicted boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "    fills = outputs.pred_fill[0, keep].cpu()\n",
    "    rotations = outputs.pred_rotation[0, keep].cpu()\n",
    "    return probas, keep, bboxes_scaled, fills, rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.6):\n",
    "  # keep only predictions with confidence >= threshold\n",
    "  \n",
    "    probas, keep, bboxes_scaled, fills, rotations = get_preds(image, outputs, threshold)\n",
    "    # plot results\n",
    "    plot_results(image, probas[keep], bboxes_scaled, fills, rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "744ded39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client\n",
    "from lib.ppt_interface import PPT_shapes, write_slide\n",
    "import random\n",
    "Application = win32com.client.Dispatch('PowerPoint.Application')\n",
    "shape_manager = PPT_shapes()\n",
    "\n",
    "def get_readout(image, outputs, threshold=0.8):\n",
    "    probas, keep, bboxes_scaled, fills, rotations = get_preds(image, outputs, threshold)\n",
    "    prob = probas[keep]\n",
    "    readout = []\n",
    "    for p, (xmin, ymin, xmax, ymax), (r,g,b), (v1,v2) in zip(prob, bboxes_scaled.tolist(), fills.tolist(), rotations.tolist()):\n",
    "            x,y, w, h = xmin, ymin, xmax - xmin, ymax - ymin\n",
    "            cl = p.argmax()\n",
    "            read = {\n",
    "                'name': id2label[cl.item()],\n",
    "                'left': x,\n",
    "                'top':y,\n",
    "                'width': w,\n",
    "                'height':h,\n",
    "                'fillRGB': [r,g,b],\n",
    "                'rotation': np.arctan2(v2,v1)\n",
    "            }\n",
    "            readout.append(read)\n",
    "    return readout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2a5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(idx, threshold):\n",
    "    pixel_values, target = train_dataset[idx]\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "    print(pixel_values.shape)\n",
    "    # forward pass to get class logits and bounding boxes\n",
    "    outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "    image_id = target['image_id'].item()\n",
    "    image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "    image = Image.open(os.path.join(f'{DATA_BASE}/train', image['file_name']))\n",
    "    img = visualize_predictions(image, outputs, threshold)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e68a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 750, 1333])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8d06b4597128>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-df1fe776845d>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(idx, threshold)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mimage_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{DATA_BASE}/train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "test(31, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96be5a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 74,\n",
       "  'image_id': 9,\n",
       "  'category_id': 0,\n",
       "  'iscrowd': 0,\n",
       "  'area': 86422.53158569336,\n",
       "  'bbox': [66.0, 28.75, 828.0, 104.37503814697266],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [1.0, 1.0, 1.0]},\n",
       " {'id': 75,\n",
       "  'image_id': 9,\n",
       "  'category_id': 1,\n",
       "  'iscrowd': 0,\n",
       "  'area': 18105.059098228812,\n",
       "  'bbox': [520.9722290039062,\n",
       "   287.4635314941406,\n",
       "   29.29732322692871,\n",
       "   617.9765625],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [1.0, 1.0, 1.0]},\n",
       " {'id': 76,\n",
       "  'image_id': 9,\n",
       "  'category_id': 6,\n",
       "  'iscrowd': 0,\n",
       "  'area': 8457.770312875276,\n",
       "  'bbox': [652.5560913085938,\n",
       "   283.0004577636719,\n",
       "   48.005985260009766,\n",
       "   176.18157958984375],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [0.0, 1.0, 0.0]},\n",
       " {'id': 77,\n",
       "  'image_id': 9,\n",
       "  'category_id': 7,\n",
       "  'iscrowd': 0,\n",
       "  'area': 12576.444551141001,\n",
       "  'bbox': [343.7273864746094,\n",
       "   241.77346801757812,\n",
       "   66.83157348632812,\n",
       "   188.18118286132812],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [1.0, 0.0, 0.0]},\n",
       " {'id': 78,\n",
       "  'image_id': 9,\n",
       "  'category_id': 2,\n",
       "  'iscrowd': 0,\n",
       "  'area': 723.8744659423828,\n",
       "  'bbox': [558.8406982421875, 134.89346313476562, 3, 241.29148864746094],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [0.0, 0.0, 0.0]},\n",
       " {'id': 79,\n",
       "  'image_id': 9,\n",
       "  'category_id': 4,\n",
       "  'iscrowd': 0,\n",
       "  'area': 20025.636732980493,\n",
       "  'bbox': [568.2567749023438,\n",
       "   359.8345031738281,\n",
       "   95.84117889404297,\n",
       "   208.94606018066406],\n",
       "  'segmentation': [],\n",
       "  'angle': 0.0,\n",
       "  'rotation': [1.0, 0.0],\n",
       "  'fill': [1.0, 0.0, 0.0]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.coco.imgToAnns[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ef497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcfb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a91fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01cf48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Presentation = Application.Presentations.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5685a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = get_readout(image, outputs, threshold=0.6)\n",
    "write_slide(readout, Presentation, shape_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Presentation.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
