{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI variables in backbone may need to be unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e37d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from jax_resnet import pretrained_resnest, Sequential, slice_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89ddd06b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceArray' object has no attribute 'expand_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a5a9191a18ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeviceArray' object has no attribute 'expand_dims'"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "random.normal(key, (10,)).expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d53d8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DetrObjectDetectionOutput(): # Todo maybe subclass from Model output\n",
    "    loss: Optional[jnp.ndarray] = None\n",
    "    loss_dict: Optional[Dict] = None\n",
    "    logits: jnp.ndarray = None\n",
    "    pred_boxes: jnp.ndarray = None\n",
    "    pred_fill: jnp.ndarray = None\n",
    "    pred_rotation: jnp.ndarray = None\n",
    "    auxiliary_outputs: Optional[List[Dict]] = None\n",
    "    last_hidden_state: Optional[jnp.ndarray] = None\n",
    "    decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n",
    "    decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n",
    "    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n",
    "    encoder_last_hidden_state: Optional[jnp.ndarray] = None\n",
    "    encoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n",
    "    encoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n",
    "        \n",
    "        \n",
    "class DetrConvModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This module adds 2D position embeddings to all intermediate feature maps of the convolutional encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conv_encoder, position_embedding):\n",
    "        super().__init__()\n",
    "        self.conv_encoder = conv_encoder\n",
    "        self.position_embedding = position_embedding\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        # send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\n",
    "        out = self.conv_encoder(pixel_values, pixel_mask)\n",
    "        pos = []\n",
    "        for feature_map, mask in out:\n",
    "            # position encoding\n",
    "            pos.append(self.position_embedding(feature_map, mask)) # TODO Check .to(feature_map.dtype)\n",
    "\n",
    "        return out, pos\n",
    "    \n",
    "def _expand_mask(mask: jnp.ndarray, dtype: jnp.dtype, tgt_len: Optional[int] = None):\n",
    "    raise NotImplemented\n",
    "    \n",
    "    \n",
    "# class DetrSinePositionEmbedding(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n",
    "#     need paper, generalized to work on images.\n",
    "#     \"\"\"\n",
    "#     raise NotImplemented\n",
    "\n",
    "#     def __init__(self, embedding_dim=64, temperature=10000, normalize=False, scale=None):\n",
    "#         super().__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.temperature = temperature\n",
    "#         self.normalize = normalize\n",
    "#         if scale is not None and normalize is False:\n",
    "#             raise ValueError(\"normalize should be True if scale is passed\")\n",
    "#         if scale is None:\n",
    "#             scale = 2 * math.pi\n",
    "#         self.scale = scale\n",
    "\n",
    "#     def forward(self, pixel_values, pixel_mask):\n",
    "#         assert pixel_mask is not None, \"No pixel mask provided\"\n",
    "#         y_embed = jnp.cumsum(pixel_mask, 1, dtype=jnp.float32)\n",
    "#         x_embed = jnp.cumsum(pixel_mask, 2, dtype=jnp.float32)\n",
    "#         if self.normalize:\n",
    "#             y_embed = y_embed / (y_embed[:, -1:, :] + 1e-6) * self.scale\n",
    "#             x_embed = x_embed / (x_embed[:, :, -1:] + 1e-6) * self.scale\n",
    "\n",
    "#         dim_t = jnp.arange(stop=self.embedding_dim, dtype=jnp.float32)\n",
    "#         dim_t = self.temperature ** (2 * (dim_t // 2) / self.embedding_dim)\n",
    "\n",
    "#         pos_x = x_embed[:, :, :, None] / dim_t\n",
    "#         pos_y = y_embed[:, :, :, None] / dim_t\n",
    "#         pos_x = jnp.stack((jnp.sin(pos_x[:, :, :, 0::2]), jnp.cos(pos_x[:, :, :, 1::2])), dim=4).flatten(3)\n",
    "#         pos_y = jnp.stack((jnp.sin(pos_y[:, :, :, 0::2]), jnp.cos(pos_y[:, :, :, 1::2])), dim=4).flatten(3)\n",
    "#         pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "#         return pos\n",
    "\n",
    "\n",
    "class DetrLearnedPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    This module learns 2D positional embeddings up to a fixed maximum size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        self.row_embeddings = nn.Embed(50, embedding_dim)\n",
    "        self.column_embeddings = nn.Embed(50, embedding_dim)\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask=None):\n",
    "        h, w = pixel_values.shape[-2:]\n",
    "        i = jnp.arange(w, device=pixel_values.device)\n",
    "        j = jnp.arange(h, device=pixel_values.device)\n",
    "        x_emb = self.column_embeddings(i)\n",
    "        y_emb = self.row_embeddings(j)\n",
    "        pos = jnp.cat([jnp.repeat(jnp.expand_dims(x_emb, 0), (h, 1, 1)), jnp.repeat(jnp.expand_dims(y_emb, 1), (1, w, 1))], dim=-1)\n",
    "        pos = jnp.permute(pos, (2, 0, 1))\n",
    "        pos = jnp.expand_dims(pos, 0)\n",
    "        pos = jnp.repeat(pos (pixel_values.shape[0], 1, 1, 1))\n",
    "        return pos\n",
    "    \n",
    "    \n",
    "def build_position_encoding(config):\n",
    "    '''\n",
    "    Builds the position encoding - half the size of d_model as we concat x_emb, y_emb\n",
    "    '''\n",
    "    n_embed = config.d_model // 2\n",
    "    if config.position_embedding_type == \"sine\":\n",
    "        # TODO find a better way of exposing other arguments\n",
    "        position_embedding = DetrSinePositionEmbedding(n_embed, normalize=True)\n",
    "    elif config.position_embedding_type == \"learned\":\n",
    "        position_embedding = DetrLearnedPositionEmbedding(n_embed)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported {config.position_embedding_type}\")\n",
    "\n",
    "    return position_embedding\n",
    "\n",
    "\n",
    "\n",
    "class DetrAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-headed attention from 'Attention Is All You Need' paper.\n",
    "\n",
    "    Here, we add position embeddings to the queries and keys (as explained in the DETR paper).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.k_proj = nn.Dense(embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Dense(embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Dense(embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Dense(embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: jnp.ndarray, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def with_pos_embed(self, tensor: jnp.ndarray, position_embeddings: Optional[Tensor]):\n",
    "        return tensor if position_embeddings is None else tensor + position_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: jnp.ndarray,\n",
    "        attention_mask: Optional[jnp.ndarray] = None,\n",
    "        position_embeddings: Optional[jnp.ndarray] = None,\n",
    "        key_value_states: Optional[jnp.ndarray] = None,\n",
    "        key_value_position_embeddings: Optional[jnp.ndarray] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[jnp.ndarray, Optional[jnp.ndarray], Optional[Tuple[jnp.ndarray]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # add position embeddings to the hidden states before projecting to queries and keys\n",
    "        if position_embeddings is not None:\n",
    "            hidden_states_original = hidden_states\n",
    "            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n",
    "\n",
    "        # add key-value position embeddings to the key value states\n",
    "        if key_value_position_embeddings is not None:\n",
    "            key_value_states_original = key_value_states\n",
    "            key_value_states = self.with_pos_embed(key_value_states, key_value_position_embeddings)\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states_original), -1, bsz)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states_original), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083a790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9f24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNeSt50, variables = pretrained_resnest(50)\n",
    "model = ResNeSt50()\n",
    "idx = 18 # gives B, 7,7,2048\n",
    "backbone, backbone_variables = Sequential(model.layers[0:idx]), slice_variables(variables, end=idx) \n",
    "output = backbone.apply(backbone_variables, jnp.ones((32, 224, 224, 3)),  # ImageNet sized inputs.\n",
    "                  mutable=False)  # Ensure `batch_stats` aren't updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc205d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 7, 7, 2048)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5492234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "    # attributes\n",
       "    layers = [ResNetDStem(\n",
       "        # attributes\n",
       "        conv_block_cls = functools.partial(<class 'jax_resnet.common.ConvBlock'>, conv_cls=<class 'flax.linen.linear.Conv'>, norm_cls=functools.partial(<class 'flax.linen.normalization.BatchNorm'>, momentum=0.9))\n",
       "        stem_width = 32\n",
       "        adaptive_first_width = False\n",
       "    ), functools.partial(<function max_pool at 0x7fa03024caf0>, window_shape=(3, 3), strides=(2, 2), padding=((1, 1), (1, 1))), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), ResNeStBottleneckBlock(\n",
       "        # attributes\n",
       "        skip_cls = ResNeStSkipConnection\n",
       "        avg_pool_first = False\n",
       "        radix = 2\n",
       "        splat_cls = functools.partial(<class 'jax_resnet.splat.SplAtConv2d'>, match_reference=True)\n",
       "    ), functools.partial(<function mean at 0x7fa0345be040>, axis=(1, 2)), Dense(\n",
       "        # attributes\n",
       "        features = 1000\n",
       "        use_bias = True\n",
       "        dtype = float32\n",
       "        precision = None\n",
       "        kernel_init = init\n",
       "        bias_init = zeros\n",
       "    )]\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbebc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c5d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b6555e",
   "metadata": {},
   "source": [
    "# Optimiser\n",
    "Ensure we freeze batch layers (don't put in opt) and batch stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://flax.readthedocs.io/en/latest/flax.optim.html#flax.optim.OptimizerDef.create\n",
    "# Freeze both     \n",
    "https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.BatchNorm.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
